{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import mean\n\nspark = SparkSession.builder.appName('Basics').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = df = sqlContext.sql (\"SELECT * FROM containsnull_csv\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#df.na.drop(thresh = 2).show() \n#df.na.drop(how = 'all').show()\n#df.na.drop(how = 'any').show()\n#df.na.drop(subset = ['Sales']).show()\n#df.na.fill('FILL VALUE').show() #fill all null string types with string'fill value'\n#df.na.fill(0).show() #fill all null integer types with integer '0'\n#df.na.fill('No Name', subset = ['Name']).show() #target 'Name' column to replace missing values with 'no name'"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["mean_val = df.select(mean(df['Sales'])).collect() # store the mean value of sales \nmean_sales = mean_val[0][0] # indexing the actual mean value from the row"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.na.fill(mean_sales, subset = ['Sales']).show() #filling the values of 'sales' with mean value of sales"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.na.fill(df.select(mean(df['Sales'])).collect()[0][0], subset = ['Sales']).show()"],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"MissingValues","notebookId":2775170003757168},"nbformat":4,"nbformat_minor":0}
