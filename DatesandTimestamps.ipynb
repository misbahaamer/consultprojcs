{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import dayofmonth, hour, dayofyear, month, year, weekofyear, format_number, date_format\n\nspark = SparkSession.builder.appName('dates').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = sqlContext.sql (\"SELECT * FROM appl_stock_csv\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#df.select(dayofmonth(df['Date']).alias('Day')).show() # show day from the 'date' column as month\n#df.select(hour(df['Date']).alias('hour')).show() #show hour from the 'date' column as hour\n#df.select(month(df['Date']).alias('Month')).show() # show month from 'date' column as month"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#df.select(year(df['Date'])).show()\n#df.withColumn(\"Year\", year(df['Date'])).show() #create a new column \"Year\" extracted from 'Date' and append it to existing df\n#newdf = df.withColumn(\"Year\", year(df['Date']))\n#newdf.groupBy(\"Year\").mean().show() # from the new \"Year\" column show mean values of all columns grouped by year\nresult = newdf.groupBy(\"Year\").mean().select([\"Year\", \"avg(Close)\"])\n#result.withColumnRenamed(\"avg(Close)\", \"Average closing price\").show()\nresult.withColumnRenamed(\"avg(Close)\", \"Average closing price\").select(['Year', format_number('Average closing price', 2).alias('Average Closing Price')]).show()"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"DatesandTimestamps","notebookId":2775170003757177},"nbformat":4,"nbformat_minor":0}
